{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a1083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing evaluation tasks for dataset: EgyptianCarPricing (task: regression) ---\n",
      "\n",
      "--- Running 30 tasks using 16 processes... ---\n"
     ]
    }
   ],
   "source": [
    "# Updated imports for a more robust test harness.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "from sklearn.datasets import (\n",
    "    load_iris,\n",
    "    load_diabetes,\n",
    "    fetch_california_housing,\n",
    "    load_wine,\n",
    "    load_digits,\n",
    "    load_breast_cancer,\n",
    "    fetch_openml,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from scipy.linalg import det\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Suppress some common warnings that can clutter the output.\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --- Epsilon-based selection methods ---\n",
    "\n",
    "def select_oips(X, eps):\n",
    "    \"\"\"\n",
    "    Online Input Point Selection (OIPS) based on RBF kernel similarity.\n",
    "    This optimized version uses an incremental lookup table for efficiency.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The dataset.\n",
    "        eps (float): The epsilon threshold for similarity.\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of the selected points.\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    if n_samples == 0:\n",
    "        return []\n",
    "\n",
    "    selected_indices = []\n",
    "\n",
    "    # This array acts as our \"lookup table\"\n",
    "    # It stores the maximum similarity found so far for each point.\n",
    "    max_sims = np.zeros(n_samples)\n",
    "\n",
    "    # Start with a random point to initialize the process.\n",
    "    start_idx = np.random.randint(n_samples)\n",
    "    selected_indices.append(start_idx)\n",
    "\n",
    "    # Pre-calculate similarities to the first selected point.\n",
    "    all_sims = pairwise_kernels(X, X[start_idx].reshape(1, -1), metric='rbf').flatten()\n",
    "    max_sims = all_sims.copy()\n",
    "\n",
    "    while True:\n",
    "        # Check if the maximum similarity to any selected point is below epsilon.\n",
    "        # This is the stopping criterion.\n",
    "        if np.max(max_sims[selected_indices]) >= eps:\n",
    "            break\n",
    "\n",
    "        # Find the next point to select: the one with the lowest max similarity.\n",
    "        remaining_indices = np.setdiff1d(np.arange(n_samples), selected_indices)\n",
    "        if len(remaining_indices) == 0:\n",
    "            break\n",
    "\n",
    "        next_idx = remaining_indices[np.argmin(max_sims[remaining_indices])]\n",
    "        selected_indices.append(next_idx)\n",
    "\n",
    "        # Update the lookup table:\n",
    "        # Calculate new similarities from the newly selected point.\n",
    "        new_sims = pairwise_kernels(X, X[next_idx].reshape(1, -1), metric='rbf').flatten()\n",
    "        # Update the max similarity for each point.\n",
    "        max_sims = np.maximum(max_sims, new_sims)\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "def select_dbscan(X, eps, min_samples=5):\n",
    "    \"\"\"\n",
    "    Selects cluster medoids using DBSCAN clustering.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The dataset.\n",
    "        eps (float): The epsilon parameter for DBSCAN.\n",
    "        min_samples (int): The minimum number of samples to form a cluster.\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of the selected medoid points.\n",
    "    \"\"\"\n",
    "    labels = DBSCAN(eps=eps, min_samples=min_samples).fit_predict(X)\n",
    "    selected_indices = []\n",
    "    for lbl in np.unique(labels):\n",
    "        if lbl == -1:\n",
    "            continue  # Skip noise points.\n",
    "        pts_indices = np.where(labels == lbl)[0]\n",
    "        pts = X[pts_indices]\n",
    "        # Find the point in the cluster closest to the cluster's mean (the medoid).\n",
    "        medoid_idx = pts_indices[np.argmin(np.linalg.norm(pts - pts.mean(axis=0), axis=1))]\n",
    "        selected_indices.append(medoid_idx)\n",
    "    return selected_indices\n",
    "\n",
    "def select_farthest(X, eps):\n",
    "    \"\"\"\n",
    "    Farthest point sampling (FPS) with an epsilon-based stopping criterion.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The dataset.\n",
    "        eps (float): The epsilon threshold for stopping.\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of the selected points.\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    if n_samples == 0:\n",
    "        return []\n",
    "\n",
    "    # Pick a random starting point.\n",
    "    start_idx = np.random.randint(n_samples)\n",
    "    selected_indices = [start_idx]\n",
    "    # Keep track of the squared distance to the nearest selected point.\n",
    "    min_dists_sq = np.linalg.norm(X - X[start_idx], axis=1)**2\n",
    "\n",
    "    while True:\n",
    "        # Find the point that is farthest from all currently selected points.\n",
    "        farthest_idx = np.argmax(min_dists_sq)\n",
    "        # Stop if the farthest point is within the epsilon threshold.\n",
    "        if np.sqrt(min_dists_sq[farthest_idx]) < eps:\n",
    "            break\n",
    "        selected_indices.append(farthest_idx)\n",
    "        # Update distances with the newly selected point.\n",
    "        new_dists_sq = np.linalg.norm(X - X[farthest_idx], axis=1)**2\n",
    "        min_dists_sq = np.minimum(min_dists_sq, new_dists_sq)\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "# --- Proposed epsilon-based method ---\n",
    "\n",
    "def compute_residuals(X, y):\n",
    "    \"\"\"\n",
    "    Computes absolute residuals from a linear regression model.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The feature matrix.\n",
    "        y (np.ndarray): The target vector.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The array of absolute residuals.\n",
    "    \"\"\"\n",
    "    # Using SGDRegressor for efficiency on large datasets.\n",
    "    model = SGDRegressor(random_state=0).fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    residuals = np.abs(y - y_pred)\n",
    "    return residuals\n",
    "\n",
    "def select_proposed(X, y, eps=0.5, max_iter=1000):\n",
    "    \"\"\"\n",
    "    The proposed selection method based on residuals and clustering.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The feature matrix.\n",
    "        y (np.ndarray): The target vector.\n",
    "        eps (float): The epsilon parameter for DBSCAN.\n",
    "        max_iter (int): Maximum number of iterations for the while loop.\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of the selected points.\n",
    "    \"\"\"\n",
    "    # 1) Precompute residuals\n",
    "    resid = compute_residuals(X, y)\n",
    "\n",
    "    # 2) Determine global percentile threshold\n",
    "    resid_thr = np.percentile(resid, 50)\n",
    "\n",
    "    selected_indices = []\n",
    "    alive = np.ones(len(X), dtype=bool)\n",
    "    it = 0\n",
    "\n",
    "    while np.any(alive) and it < max_iter:\n",
    "        it += 1\n",
    "\n",
    "        # 3) Mask of points valid this iteration\n",
    "        # Select points below the residual threshold and not yet selected.\n",
    "        valid_indices = np.where(alive & (resid <= resid_thr))[0]\n",
    "        if valid_indices.size == 0:\n",
    "            break\n",
    "\n",
    "        # 4) Cluster only the valid points\n",
    "        X_valid = X[valid_indices]\n",
    "        try:\n",
    "            # Set a fixed, robust min_samples value.\n",
    "            labels = DBSCAN(eps=eps, min_samples=5).fit_predict(X_valid)\n",
    "        except ValueError:\n",
    "            break\n",
    "\n",
    "        # 5) Pick one representative per non-noise cluster\n",
    "        new_indices = []\n",
    "        for lbl in np.unique(labels):\n",
    "            if lbl == -1:  # Noise points are not selected.\n",
    "                continue\n",
    "\n",
    "            cluster_indices_in_valid = np.where(labels == lbl)[0]\n",
    "            orig_indices_in_cluster = valid_indices[cluster_indices_in_valid]\n",
    "\n",
    "            # Select the point closest to the cluster mean.\n",
    "            cluster_points = X[orig_indices_in_cluster]\n",
    "            medoid_idx = orig_indices_in_cluster[np.argmin(np.linalg.norm(cluster_points - cluster_points.mean(axis=0), axis=1))]\n",
    "            new_indices.append(medoid_idx)\n",
    "\n",
    "        selected_indices.extend(new_indices)\n",
    "\n",
    "        # 6) Remove all clustered points for the next iteration\n",
    "        alive[valid_indices] = False\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "# --- Fixed-budget selection methods ---\n",
    "\n",
    "def select_kmeans(X, y, M, task):\n",
    "    \"\"\"\n",
    "    Selects cluster centroids from KMeans clustering.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The feature matrix.\n",
    "        y (np.ndarray): The target vector (not used for selection).\n",
    "        M (int): The number of points to select.\n",
    "        task (str): The task type ('classification' or 'regression').\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of the selected cluster centroids.\n",
    "    \"\"\"\n",
    "    M = min(M, len(X))\n",
    "    if M == 0: return []\n",
    "\n",
    "    km = KMeans(n_clusters=M, random_state=0, n_init='auto').fit(X)\n",
    "\n",
    "    # Find the representative point for each cluster (the one closest to the center).\n",
    "    selected_indices = []\n",
    "    for i in range(M):\n",
    "        cluster_indices = np.where(km.labels_ == i)[0]\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "        center = km.cluster_centers_[i]\n",
    "        distances = np.linalg.norm(X[cluster_indices] - center, axis=1)\n",
    "        selected_indices.append(cluster_indices[np.argmin(distances)])\n",
    "    return selected_indices\n",
    "\n",
    "def select_random(X, y, M, task):\n",
    "    \"\"\"\n",
    "    Randomly selects M points from the dataset.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The feature matrix.\n",
    "        y (np.ndarray): The target vector (not used for selection).\n",
    "        M (int): The number of points to select.\n",
    "        task (str): The task type (not used for selection).\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of the selected points.\n",
    "    \"\"\"\n",
    "    return list(np.random.choice(len(X), min(M, len(X)), replace=False))\n",
    "\n",
    "\n",
    "# --- Helper functions for a more robust evaluation harness ---\n",
    "\n",
    "def get_datasets():\n",
    "    \"\"\"\n",
    "    Loads and returns a dictionary of various datasets.\n",
    "\n",
    "    The Egyptian Used Car Pricing dataset requires specific preprocessing\n",
    "    to handle categorical and numeric data correctly.\n",
    "    \"\"\"\n",
    "    # Load the Egyptian Used Car Pricing dataset from a local file.\n",
    "    try:\n",
    "        df = pd.read_csv('Egypt-Used-Car-Price.csv')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset from local file. Please ensure the path is correct. Error: {e}\")\n",
    "        return {}\n",
    "\n",
    "    # Separate features and target\n",
    "    y = df['Price']\n",
    "    X = df.drop('Price', axis=1)\n",
    "\n",
    "    # --- Feature Engineering for EgyptianCarPricing dataset ---\n",
    "    # Add car age as a new feature.\n",
    "    current_year = datetime.datetime.now().year\n",
    "    X['Car_Age'] = current_year - X['Year']\n",
    "    # Drop the original 'Year' column, as it's now represented by 'Car_Age'.\n",
    "    X = X.drop('Year', axis=1)\n",
    "    # Drop Engine_CC as it's not a common feature\n",
    "    X = X.drop('Engine_CC', axis=1)\n",
    "\n",
    "    # Convert categorical columns to numeric using one-hot encoding\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    # Convert all columns to a numeric format if they are not already.\n",
    "    # This handles any remaining mixed data types.\n",
    "    X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "    return {\n",
    "        'EgyptianCarPricing': ('regression', (X.values, y.values))\n",
    "    }\n",
    "\n",
    "def find_optimal_epsilon(X, k=5):\n",
    "    \"\"\"\n",
    "    Finds a suitable epsilon for DBSCAN by calculating the average distance\n",
    "    to the k-th nearest neighbor. This is a common, reproducible heuristic.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The dataset.\n",
    "        k (int): The number of neighbors to consider.\n",
    "\n",
    "    Returns:\n",
    "        float: A suitable epsilon value for DBSCAN.\n",
    "    \"\"\"\n",
    "    if len(X) < k:\n",
    "        return 0.1 # Fallback for very small datasets.\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(X)\n",
    "    distances, _ = neigh.kneighbors(X)\n",
    "\n",
    "    # We'll use the median of the k-th distances as a simple, robust heuristic.\n",
    "    # Multiply by a small factor to make epsilon more generous.\n",
    "    median_kth_distance = np.median(distances[:, -1])\n",
    "\n",
    "    return 1.5 * median_kth_distance\n",
    "\n",
    "\n",
    "def evaluate_selection_method(args):\n",
    "    \"\"\"\n",
    "    Evaluates a single selection method on a given dataset.\n",
    "    This function is designed to be run by a multiprocessing worker.\n",
    "    \"\"\"\n",
    "    name, fn, X_tr, y_tr, X_te, y_te, task, M_budget, eps_val, dataset_name = args\n",
    "    print(f\"Starting {name} on {dataset_name}...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Select points based on the method and its parameters.\n",
    "    selected_indices = []\n",
    "    if name == 'Proposed':\n",
    "        selected_indices = fn(X_tr, y_tr, eps=eps_val)\n",
    "        M_used = len(selected_indices)\n",
    "    elif name in ['DBSCAN_eps', 'OIPS', 'Farthest']:\n",
    "        selected_indices = fn(X_tr, eps=eps_val)\n",
    "        M_used = len(selected_indices)\n",
    "    else:  # Fixed-budget methods\n",
    "        selected_indices = fn(X_tr, y_tr, M=M_budget, task=task)\n",
    "        M_used = len(selected_indices)\n",
    "\n",
    "    selection_time = time.time() - start_time\n",
    "\n",
    "    # Handle cases where no points are selected or a single class is present.\n",
    "    if M_used == 0 or (task == 'classification' and len(np.unique(y_tr[selected_indices])) < 2):\n",
    "        print(f\"Finished {name} on {dataset_name} with 0 selected points or insufficient classes.\")\n",
    "        return {\n",
    "            'Dataset': dataset_name, 'Method': name, 'Task': task, 'M_used': M_used, 'NumPoints': 0,\n",
    "            'SelTime(s)': selection_time, 'TrainTime(s)': None, 'TestTime(s)': None,\n",
    "            'Accuracy': None, 'R2': None, 'MSE': None,\n",
    "            'MAE': None, 'RMSE': None,\n",
    "            'budget': M_budget, 'epsilon': eps_val\n",
    "        }\n",
    "\n",
    "    # Use the selected points to train the model.\n",
    "    Zx, Zy = X_tr[selected_indices], y_tr[selected_indices]\n",
    "\n",
    "    start_train_time = time.time()\n",
    "    if task == 'classification':\n",
    "        model = GaussianProcessClassifier(RBF(), copy_X_train=False).fit(Zx, Zy)\n",
    "    else:\n",
    "        model = GaussianProcessRegressor(RBF(), copy_X_train=False).fit(Zx, Zy)\n",
    "    train_time = time.time() - start_train_time\n",
    "\n",
    "    # Evaluate the trained model.\n",
    "    start_test_time = time.time()\n",
    "    preds = model.predict(X_te)\n",
    "    test_time = time.time() - start_test_time\n",
    "\n",
    "    acc, r2, mse, mae, rmse = None, None, None, None, None\n",
    "    if task == 'classification':\n",
    "        acc = accuracy_score(y_te, preds)\n",
    "    else:\n",
    "        r2 = r2_score(y_te, preds)\n",
    "        mse = mean_squared_error(y_te, preds)\n",
    "        mae = mean_absolute_error(y_te, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        tol = 0.1 * np.std(y_tr)\n",
    "        acc = np.mean(np.abs(preds - y_te) < tol)\n",
    "\n",
    "    print(f\"Finished {name} on {dataset_name}.\")\n",
    "\n",
    "    return {\n",
    "        'Dataset': dataset_name, 'Method': name, 'Task': task, 'M_used': M_used, 'NumPoints': M_used,\n",
    "        'SelTime(s)': round(selection_time, 4), 'TrainTime(s)': round(train_time, 4), 'TestTime(s)': round(test_time, 4),\n",
    "        'Accuracy': round(acc, 4) if acc is not None else None,\n",
    "        'R2': round(r2, 4) if r2 is not None else None,\n",
    "        'MSE': round(mse, 4) if mse is not None else None,\n",
    "        'MAE': round(mae, 4) if mae is not None else None,\n",
    "        'RMSE': round(rmse, 4) if rmse is not None else None,\n",
    "        'budget': M_used, 'epsilon': eps_val\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = []\n",
    "    datasets = get_datasets()\n",
    "\n",
    "    method_functions = {\n",
    "        'Proposed': select_proposed,\n",
    "        'DBSCAN_eps': select_dbscan,\n",
    "        'OIPS': select_oips,\n",
    "        'Farthest': select_farthest,\n",
    "        'KMeans': select_kmeans,\n",
    "        'Random': select_random,\n",
    "    }\n",
    "\n",
    "    all_tasks = []\n",
    "\n",
    "    for dataset_name, (task, (X, y)) in datasets.items():\n",
    "        print(f\"--- Preparing evaluation tasks for dataset: {dataset_name} (task: {task}) ---\")\n",
    "\n",
    "        # Data splitting and scaling for features.\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "        x_scaler = StandardScaler()\n",
    "        X_tr_scaled = x_scaler.fit_transform(X_tr)\n",
    "        X_te_scaled = x_scaler.transform(X_te)\n",
    "\n",
    "        # Dynamically find a suitable epsilon using a data-driven heuristic.\n",
    "        optimal_eps = find_optimal_epsilon(X_tr_scaled)\n",
    "\n",
    "        # Create a relevant range of epsilons around the optimal one.\n",
    "        # This is a key change for robustness and a more thorough evaluation.\n",
    "        eps_multipliers = [0.1, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 5.0, 10.0]\n",
    "        eps_values_to_test = [optimal_eps * m for m in eps_multipliers]\n",
    "\n",
    "        # Use the optimal epsilon to get the budget for this dataset.\n",
    "        for current_eps in eps_values_to_test:\n",
    "            selected_indices_proposed = select_proposed(X_tr_scaled, y_tr, eps=current_eps)\n",
    "            M_used = len(selected_indices_proposed)\n",
    "\n",
    "            # Add all tasks for this dataset to the list.\n",
    "            # This will now be run in a single pass using multiprocessing.\n",
    "            for name in ['Proposed']:\n",
    "                 # Farthest and OIPS do not require the y_tr, so we pass in None\n",
    "                if name == 'Proposed':\n",
    "                    selected_indices_proposed = select_proposed(X_tr_scaled, y_tr, eps=current_eps)\n",
    "                    M_used = len(selected_indices_proposed)\n",
    "                    all_tasks.append((name, method_functions[name], X_tr_scaled, y_tr, X_te_scaled, y_te, task, M_used, current_eps, dataset_name))\n",
    "                else:\n",
    "                    all_tasks.append((name, method_functions[name], X_tr_scaled, y_tr, X_te_scaled, y_te, task, None, current_eps, dataset_name))\n",
    "\n",
    "            for name in ['KMeans', 'Random']:\n",
    "                all_tasks.append((name, method_functions[name], X_tr_scaled, y_tr, X_te_scaled, y_te, task, M_used, None, dataset_name))\n",
    "\n",
    "    # Use a process pool to run all tasks in parallel.\n",
    "    print(f\"\\n--- Running {len(all_tasks)} tasks using {cpu_count()} processes... ---\")\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.map(evaluate_selection_method, all_tasks)\n",
    "\n",
    "    # Convert results to a pandas DataFrame and print as a markdown table.\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\n\\n--- Final Results ---\")\n",
    "    print(df.to_markdown(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
